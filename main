"""
@author:32369
@file:test.py
@time:2021/02/04
"""
import datetime
import time

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)


# criteo是非常经典的点击率预估比赛（数据集下载）。训练集4千万行，特征连续型的有13个，类别型的26个，
# 没有提供特征名称，样本按时间排序。测试集6百万行。

class DeepFM(nn.Module):
    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8,
                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]):
        """
        cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表
        nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况
        """
        super().__init__()
        self.cate_fea_size = len(cate_fea_nuniqs)
        self.nume_fea_size = nume_fea_size

        """FFM部分"""
        # 一阶
        if self.nume_fea_size != 0:
            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示

        self.fm_1st_order_sparse_emb = torch.nn.Embedding(sum(cate_fea_nuniqs), 1)  # [40,1]
        self.bias = torch.nn.Parameter(torch.zeros((1,)))
        self.offsets = np.array((0, *np.cumsum(cate_fea_nuniqs)[:-1]), dtype=np.long)  # 类别特征的一阶表示

        # 二阶
        self.embeddings = torch.nn.ModuleList([
            torch.nn.Embedding(sum(cate_fea_nuniqs), emb_size) for _ in range(self.cate_fea_size)  # 类别特征的二阶表示
        ])
        self.offsets = np.array((0, *np.cumsum(cate_fea_nuniqs)[:-1]), dtype=np.long)
        for embedding in self.embeddings:
            torch.nn.init.xavier_uniform_(embedding.weight.data)

        """DNN部分"""
        self.all_dims = [self.cate_fea_size*self.cate_fea_size * emb_size] + hid_dims
        self.dense_linear = nn.Linear(self.nume_fea_size,
                                      self.cate_fea_size * self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致
        self.relu = nn.ReLU()
        # for DNN
        for i in range(1, len(self.all_dims)):
            setattr(self, 'linear_' + str(i), nn.Linear(self.all_dims[i - 1], self.all_dims[i]))
            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(self.all_dims[i]))
            setattr(self, 'activation_' + str(i), nn.ReLU())
            setattr(self, 'dropout_' + str(i), nn.Dropout(dropout[i - 1]))
        # for output
        self.dnn_linear = nn.Linear(hid_dims[-1], num_classes)
        self.sigmoid = nn.Sigmoid()

    def forward(self, X_sparse, X_dense=None):
        """           【2048 26】    【 2048 13】
        X_sparse: 类别型特征输入  [bs, cate_fea_size]
        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]
        """
        X_sparse = X_sparse + X_sparse.new_tensor(self.offsets, dtype=np.long).unsqueeze(0)

        """FM 一阶部分"""
        fm_1st_sparse_emb = self.fm_1st_order_sparse_emb(X_sparse)  # [2048 26 1]
        fm_1st_sparse_res = torch.sum(fm_1st_sparse_emb, dim=1) + self.bias  # [2048 1]

        if X_dense is not None:
            fm_1st_dense_res = self.fm_1st_order_dense(X_dense)  # [2048 1]
            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res  # [2048 1]
        else:
            fm_1st_part = fm_1st_sparse_res  # [2048, 1]

        """FM 二阶部分"""
        xs = [self.embeddings[i](X_sparse) for i in range(self.cate_fea_size)]  # [26, 2048 ,26 ,8]
        ix = list()
        for i in range(self.cate_fea_size - 1):
            for j in range(i + 1, self.cate_fea_size):
                ix.append(xs[j][:, i] * xs[i][:, j])
        ix = torch.stack(ix, dim=1)  # [325 2048 8]
        fm_2nd_part = torch.sum(torch.sum(ix, dim=1), dim=1, keepdim=True)  # [2048 1]
        """DNN部分"""
        fm_2nd_concat_1d = torch.cat(xs, dim=1)  # [2048, 26*26, 8]
        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)  # [2048, 26*26*8]

        if X_dense is not None:
            dense_out = self.relu(self.dense_linear(X_dense))  # [2048, 26*26*8]
            dnn_out = dnn_out + dense_out  # [2048, 26*26*8]

        for i in range(1, len(self.all_dims)):
            dnn_out = getattr(self, 'linear_' + str(i))(dnn_out)
            dnn_out = getattr(self, 'batchNorm_' + str(i))(dnn_out)
            dnn_out = getattr(self, 'activation_' + str(i))(dnn_out)
            dnn_out = getattr(self, 'dropout_' + str(i))(dnn_out)

        dnn_out = self.dnn_linear(dnn_out)  # [2048, 1]
        out = fm_1st_part + fm_2nd_part + dnn_out  # [2048, 1]
        out = self.sigmoid(out)
        return out


data = pd.read_csv("criteo_sample_50w.csv")

dense_features = [f for f in data.columns.tolist() if f[0] == "I"]
sparse_features = [f for f in data.columns.tolist() if f[0] == "C"]

data[sparse_features] = data[sparse_features].fillna('-10086', )
data[dense_features] = data[dense_features].fillna(0, )
target = ['label']

for feat in tqdm(sparse_features):
    lbe = LabelEncoder()
    data[feat] = lbe.fit_transform(data[feat])

for feat in tqdm(dense_features):
    mean = data[feat].mean()
    std = data[feat].std()
    data[feat] = (data[feat] - mean) / (std + 1e-12)

print(data.shape)
data.head()

train, valid = train_test_split(data, test_size=0.2, random_state=2020)
print(train.shape, valid.shape)

train_dataset = Data.TensorDataset(torch.LongTensor(train[sparse_features].values),
                                   torch.FloatTensor(train[dense_features].values),
                                   torch.FloatTensor(train['label'].values), )

train_loader = Data.DataLoader(dataset=train_dataset, batch_size=2048, shuffle=True)

valid_dataset = Data.TensorDataset(torch.LongTensor(valid[sparse_features].values),
                                   torch.FloatTensor(valid[dense_features].values),
                                   torch.FloatTensor(valid['label'].values), )
valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=4096, shuffle=False)

# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
device = torch.device('cpu')
print(device)
cate_fea_nuniqs = [data[f].nunique() for f in sparse_features]
model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))
model.to(device)
loss_fcn = nn.BCELoss()  # Loss函数
loss_fcn = loss_fcn.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)


# 打印模型参数
def get_parameter_number(model):
    total_num = sum(p.numel() for p in model.parameters())
    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return {'Total': total_num, 'Trainable': trainable_num}


print(get_parameter_number(model))



def train_and_eval(model, train_loader, valid_loader, epochs, device):
    best_auc = 0.0
    for _ in range(epochs):
        """训练部分"""
        model.train()
        print("Current lr : {}".format(optimizer.state_dict()['param_groups'][0]['lr']))
        train_loss_sum = 0.0
        start_time = time.time()
        for idx, x in enumerate(train_loader):
            cate_fea, nume_fea, label = x[0], x[1], x[2]
            cate_fea, nume_fea, label = cate_fea.to(device), nume_fea.to(device), label.float().to(device)
            pred = model(cate_fea, nume_fea).view(-1)
            loss = loss_fcn(pred, label)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss_sum += loss.cpu().item()
        scheduler.step()
        """推断部分"""
        model.eval()
        with torch.no_grad():
            valid_labels, valid_preds = [], []
            for idx, x in tqdm(enumerate(valid_loader)):
                cate_fea, nume_fea, label = x[0], x[1], x[2]
                cate_fea, nume_fea = cate_fea.to(device), nume_fea.to(device)
                pred = model(cate_fea, nume_fea).reshape(-1).data.cpu().numpy().tolist()
                valid_preds.extend(pred)
                valid_labels.extend(label.cpu().numpy().tolist())
        cur_auc = roc_auc_score(valid_labels, valid_preds)
        if cur_auc > best_auc:
            best_auc = cur_auc
            torch.save(model.state_dict(), "data/deepfm_best.pth")


train_and_eval(model, train_loader, valid_loader, 30, device)
